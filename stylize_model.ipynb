{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import tqdm\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pyedflib\n",
    "from scipy import signal\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Conv1D, Conv1DTranspose, LeakyReLU, Lambda\n",
    "from keras.layers import Input, Multiply, LayerNormalization, GlobalAveragePooling1D, Softmax, Add\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Time data\n",
    "DATASET_PATH = f'/***/TEST_SET_3600/'\n",
    "TARGET       = 'shhs1-200006'\n",
    "\n",
    "hyp_path = os.path.join(DATASET_PATH, TARGET, f'{TARGET}_hyp_FIR.npz')\n",
    "edf_path = os.path.join(DATASET_PATH, TARGET, f'{TARGET}_FIR.edf')\n",
    "with pyedflib.EdfReader(edf_path) as edf:\n",
    "    signal_headers = [edf.getSignalHeader(0)]\n",
    "    signal_headers[0]['sample_rate'] = 100\n",
    "    header = edf.getHeader()\n",
    "    hyp = np.load(hyp_path)['arr_0']\n",
    "    eeg = edf.readSignal(0).reshape((len(hyp), -1, 1))\n",
    "print(edf_path)\n",
    "print(hyp.shape, eeg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stage_label = ['W', 'N1', 'N2', 'N3', 'R']\n",
    "\n",
    "# for plot\n",
    "_ylim1 = [-150., 150.]\n",
    "_ylim3 = [0., 30.]\n",
    "_width = 1/18\n",
    "_alpha = .8\n",
    "_fontsize = 30\n",
    "\n",
    "five_stage_idx = [-1] * 5\n",
    "for i in range(len(hyp)):\n",
    "    for j in range(5):\n",
    "        if hyp[i] == j and five_stage_idx[j] == -1:\n",
    "            five_stage_idx[j] = i\n",
    "            break\n",
    "five_stage_idx[2] = 212\n",
    "five_stage_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "        intra_op_parallelism_threads=1,\n",
    "        inter_op_parallelism_threads=1\n",
    "    )\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "def _fft(data):\n",
    "    Fs = 120\n",
    "    data = data.ravel()\n",
    "    N = len(data)\n",
    "    if N > 4000:\n",
    "        print('Seems to be some error; got data length:', N)\n",
    "        return\n",
    "    window = np.hanning(N)\n",
    "\n",
    "    freq = np.fft.fftfreq(N, 1.0/Fs)\n",
    "    F = np.fft.fft(data * window)\n",
    "    F = abs(F)/(N/2)\n",
    "    F[0] = F[0]/2\n",
    "    return freq[:N//2], F[:N//2] * 1/(sum(window)/2/N)\n",
    "\n",
    "def print_fft(data, idx=29):\n",
    "    \"\"\"comparison between raw/reconstructed data\"\"\"    \n",
    "    data = data[idx].ravel()\n",
    "    \n",
    "    x_fft, y_fft = _fft(data)\n",
    "\n",
    "    _, ax = plt.subplots(1, 2, figsize=(24, 4))\n",
    "    ax[0].plot(data)\n",
    "    ax[0].set_title(f'Raw data: {_stage_label[int(y_test[idx])]}')\n",
    "    ax[1].plot(x_fft, y_fft, color='tab:orange')\n",
    "    ax[1].set_title('FFT(Raw data)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyAlt(keras.layers.Layer):\n",
    "    def __init__(self, alpha=0.3, **kwargs):\n",
    "        super(LeakyAlt, self).__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return -K.relu(-inputs, alpha=self.alpha)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'alpha'      : self.alpha\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StylizeModel:\n",
    "    def __init__(self, eeg, hyp, model=None) -> None:\n",
    "        if model is None:\n",
    "            self.model = keras.models.load_model('./model/model_7746.h5', custom_objects={'LeakyReLU': LeakyReLU, 'LeakyAlt': LeakyAlt, 'root_mean_squared_error': StylizeModel.root_mean_squared_error})\n",
    "            print('Model loaded')\n",
    "        else:\n",
    "            self.model = model\n",
    "        self.eeg = eeg\n",
    "        self.hyp = hyp\n",
    "        self.encoder, self.decoder = self._get_en_decoder()\n",
    "\n",
    "        if self.eeg is not None:\n",
    "            self.latent = self.encoder.predict(self.eeg)\n",
    "        w, _ = self.decoder.get_layer(name='squeeze').get_weights()\n",
    "        self.n_mfilter = w.shape[1] # w.shape: (kernel_size, f_in, f_out)\n",
    "        self.edge = self.encoder.output_shape[-1] - self.n_mfilter\n",
    "        self.imp_ascending = self._get_importance(load=True)\n",
    "\n",
    "        self.rmse_transition = self.rec_transition = None\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"show stylize model's performance\"\"\"\n",
    "        pred_test_oh, pred_test_wave = self.model.predict(x_test)\n",
    "        pred_test_score = np.argmax(pred_test_oh, axis=-1)\n",
    "\n",
    "        print(pd.DataFrame(confusion_matrix(y_test, pred_test_score), columns=_stage_label, index=_stage_label))\n",
    "        print(pd.DataFrame(confusion_matrix(y_test, pred_test_score, normalize='true'), columns=_stage_label, index=_stage_label))\n",
    "            \n",
    "        print(classification_report(y_test, pred_test_score, target_names=_stage_label, digits=4, zero_division=0))\n",
    "\n",
    "        print_fft(x_test)\n",
    "        print_fft(pred_test_wave)\n",
    "        \n",
    "    def _get_en_decoder(self):\n",
    "        StylizeModel.init_seed()\n",
    "        kernel_size = 32\n",
    "        inputs       = Input((3600, 1))\n",
    "        x_1          = self.model.get_layer(name=f'ord_{kernel_size}_1')(inputs)\n",
    "        x_2          = self.model.get_layer(name=f'alt_{kernel_size}_1')(inputs)\n",
    "        x_1          = self.model.get_layer(name=f'ord_{kernel_size}_2')(x_1)\n",
    "        x_2          = self.model.get_layer(name=f'alt_{kernel_size}_2')(x_2)\n",
    "        outputs      = Add()([x_1, x_2])\n",
    "        encoder = keras.Model(inputs, outputs)\n",
    "\n",
    "        inputs       = Input(shape=encoder.output_shape[1:])\n",
    "        mask_layer   = Input(shape=encoder.output_shape[1:])\n",
    "        x            = Multiply()([inputs, mask_layer])\n",
    "        enc          = self.model.get_layer(name=f'dec_{kernel_size}')(x)\n",
    "        enc          = self.model.get_layer(name='rec')(enc)\n",
    "        scoring      = self.model.get_layer(name='lambda')(x)\n",
    "        scoring      = self.model.get_layer(name='ln')(scoring)\n",
    "        scoring      = self.model.get_layer(name='squeeze')(scoring)\n",
    "        scoring      = GlobalAveragePooling1D()(scoring)\n",
    "        scoring      = Softmax(name='scoring')(scoring)\n",
    "        decoder = keras.Model([inputs, mask_layer], [scoring, enc])\n",
    "        \n",
    "        return encoder, decoder\n",
    "        \n",
    "    def _get_importance(self, load=True, plot=False):\n",
    "        imp_ascending = []\n",
    "        if load:\n",
    "            imp_ascending = [16, 23, 17, 2, 5, 7, 4, 25, 20, 21, 12, 15, 22, 26, 10, 24, 11, 27, 19, 9, 1, 8, 14, 18, 13, 3, 6, 0]\n",
    "        else:\n",
    "            l_test = self.encoder.predict(x_test)\n",
    "            for _ in tqdm.tqdm(range(self.n_mfilter)):\n",
    "                pred_acc = []\n",
    "                for i in range(self.n_mfilter):\n",
    "                    # no need to calculate\n",
    "                    if i in imp_ascending:\n",
    "                        pred_acc.append(0)\n",
    "                        continue\n",
    "                    # set mask\n",
    "                    mask = np.ones_like(l_test)\n",
    "                    mask[:, :, self.edge//2+i] = 0\n",
    "                    for l in imp_ascending:\n",
    "                        mask[:, :, self.edge//2+l] = 0\n",
    "                    # evaluation\n",
    "                    pred_oh = self.decoder.predict([l_test, mask])[0]\n",
    "                    pred_acc.append(accuracy_score(y_test, np.argmax(pred_oh, axis=-1)))\n",
    "\n",
    "                # save importance\n",
    "                imp_ascending.append(np.argmax(pred_acc))\n",
    "                \n",
    "                if plot:\n",
    "                    # plot\n",
    "                    _, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "                    ax.plot(pred_acc)\n",
    "                    ax.scatter(np.argmax(pred_acc), np.max(pred_acc), c='r')\n",
    "                    ax.set_ylim([0., .9])\n",
    "                    ax.set_ylabel('Accuracy')\n",
    "                    ax.set_xticks(range(self.n_mfilter))\n",
    "                    ax.grid()\n",
    "                    plt.show()\n",
    "        return imp_ascending\n",
    "    \n",
    "    def make_edf(self, dir_name=''):\n",
    "        \"\"\"save stylized signal to EDF\"\"\"\n",
    "        for i in range(len(self.imp_ascending)+1):\n",
    "            mask = np.ones_like(self.latent)\n",
    "            for j in range(i):\n",
    "                mask[:, :, self.edge//2+self.imp_ascending[j]] = 0\n",
    "            pred_wave = self.decoder.predict([self.latent, mask])[1]\n",
    "            \n",
    "            pred_wave = signal.resample_poly(pred_wave, 100, 120, axis=1)\n",
    "            pred_wave = pred_wave.reshape((1, -1)) # (n_channels, len_data)\n",
    "            \n",
    "            base = f'squeeze_{i}'\n",
    "            dir_path = os.path.join('REC', dir_name, TARGET, base) # REC/dir_name/shhs1-200006/squeeze_0\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            with pyedflib.EdfWriter(os.path.join(dir_path, f'{base}.edf'), n_channels=1, file_type=pyedflib.FILETYPE_EDF) as f:\n",
    "                f.setSignalHeaders(signal_headers)\n",
    "                f.setHeader(header)\n",
    "                f.writeSamples(pred_wave)\n",
    "            shutil.copyfile('/***/shhs1-200006_hyp_FIR.npz',\n",
    "                            os.path.join(dir_path, 'shhs1-200006_hyp.npz'))\n",
    "            print('Saved to', dir_path)\n",
    "            \n",
    "    def _get_transition(self):\n",
    "        self.rmse_transition = []\n",
    "        self.rec_transition = []\n",
    "        for i in tqdm.tqdm(range(len(self.imp_ascending)+1), desc='### calculate transition'):\n",
    "            mask = np.ones_like(self.latent)\n",
    "            for j in range(i):\n",
    "                mask[:, :, self.edge//2+self.imp_ascending[j]] = 0\n",
    "            pred_wave = self.decoder.predict([self.latent, mask])[1]\n",
    "            \n",
    "            rmse_val = StylizeModel.root_mean_squared_error(eeg, pred_wave)\n",
    "            self.rmse_transition.append(rmse_val.numpy())\n",
    "            \n",
    "            rec = np.array([pred_wave[j] for j in five_stage_idx]) # (5, 3600, 1)\n",
    "            self.rec_transition.append(rec)\n",
    "        \n",
    "    def show_rmse_transition(self):\n",
    "        if self.rmse_transition is None:\n",
    "            self._get_transition()\n",
    "\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        plt.plot(self.rmse_transition)\n",
    "        plt.xticks(range(len(self.rmse_transition)))\n",
    "        plt.xlabel('Number of latent vector removed', fontsize=20)\n",
    "        plt.ylabel('RMSE', fontsize=20)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    def show_rec_transition(self, row=1):\n",
    "        if self.rec_transition is None:\n",
    "            self._get_transition()\n",
    "            \n",
    "        def _plot_some(ax, data, label, c=None):\n",
    "            ax.plot(data, label=label, c=c)\n",
    "            ax.set_ylim(_ylim1)\n",
    "            x_fft, y_fft = _fft(data.ravel())\n",
    "            ax3 = ax.twinx().twiny()\n",
    "            ax3.plot(x_fft, y_fft, c='tab:orange', label=f'FFT({label})')\n",
    "            ax3.set_ylim(_ylim3)\n",
    "            h1, l1 = ax.get_legend_handles_labels()\n",
    "            h3, l3 = ax3.get_legend_handles_labels()\n",
    "            ax.legend(h1 + h3, l1 + l3)\n",
    "            plt.tight_layout()\n",
    "\n",
    "        _, ax = plt.subplots(1+row, 5, figsize=(30, 4*(1+row)), sharey='row')\n",
    "        if 'x_test' in globals():\n",
    "            for i, idx in enumerate(five_stage_idx):\n",
    "                ax1 = ax[0][i]\n",
    "                _plot_some(ax1, x_test[idx], label='raw')\n",
    "                ax1.set_title(f'Raw data, {_stage_label[i]}', fontsize=_fontsize)\n",
    "        for i in tqdm.tqdm(range(row), desc='### plot transition'):\n",
    "            for j in range(5):\n",
    "                ax1 = ax[i+1][j]\n",
    "                _plot_some(ax1, self.rec_transition[i][j], label='rec', c='navy') # specify threshold\n",
    "                ax1.set_title(f'Removed {i}th({self.imp_ascending[i]}), {_stage_label[j]}', fontsize=_fontsize)\n",
    "        plt.show()\n",
    "        \n",
    "    def compare(self, idx, extent):\n",
    "        \"\"\"comparison of raw/rec signals\"\"\"\n",
    "        print(f'### Signal index: {idx}, stylized to {extent}/{self.n_mfilter}')\n",
    "        \n",
    "        mask = np.ones_like(self.latent)\n",
    "        for j in range(extent):\n",
    "            mask[:, :, self.edge//2+self.imp_ascending[j]] = 0\n",
    "        stylized = self.decoder.predict([self.latent, mask])[1]\n",
    "\n",
    "        _, ax = plt.subplots(2, 1, figsize=(20, 6), sharey='row')\n",
    "        ax1 = ax[0]\n",
    "        ax2 = ax1.twinx()\n",
    "        ax3 = ax2.twiny()\n",
    "        if 'x_test' in globals():\n",
    "            ax1.plot(x_test[idx], label='raw')\n",
    "            x_fft, y_fft = _fft(x_test[idx].ravel())\n",
    "            ax3.bar(x_fft, y_fft, color='tab:orange', width=_width, alpha=_alpha)\n",
    "\n",
    "        ax1.set_xticks(np.arange(0, 3600+1, 600))\n",
    "        ax1.set_ylim(_ylim1)\n",
    "        ax3.set_xlabel('frequency[Hz]', fontsize=_fontsize)\n",
    "        ax3.set_ylim(_ylim3)\n",
    "        plt.tight_layout()\n",
    "        ##################################\n",
    "        ax1 = ax[1]\n",
    "        ax2 = ax1.twinx()\n",
    "        ax3 = ax2.twiny()\n",
    "        ax1.plot(stylized[idx], c='navy')\n",
    "        x_fft, y_fft = _fft(stylized[idx].ravel())\n",
    "        ax3.bar(x_fft, y_fft, color='tab:orange', width=_width, alpha=_alpha)\n",
    "\n",
    "        ax1.set_xticks(np.arange(0, 3600+1, 600))\n",
    "        ax1.set_ylim(_ylim1)\n",
    "        ax1.set_xlabel(f'Time[s]', fontsize=_fontsize)\n",
    "        ax3.set_ylim(_ylim3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def show_ttp(self):\n",
    "        \"\"\"show TTP(tend to predict) and plot\"\"\"\n",
    "        self.sensitive_to = [set(), set(), set(), set(), set()]\n",
    "        self.ttp = {}\n",
    "\n",
    "        for i, imp in tqdm.tqdm(enumerate(self.imp_ascending), desc='calculate ttp       ', total=self.n_mfilter):\n",
    "            mask = np.zeros_like(self.latent)\n",
    "            mask[:, :, self.edge//2+imp] = 1\n",
    "            pred_oh = self.decoder.predict([self.latent, mask])[0]\n",
    "            \n",
    "            cm = confusion_matrix(hyp, np.argmax(pred_oh, axis=-1))\n",
    "            pred_amb = np.argmax(np.sum(cm, axis=0))\n",
    "            self.sensitive_to[pred_amb].add(i)\n",
    "            self.ttp[i] = _stage_label[pred_amb]\n",
    "\n",
    "        print('imp_ascending:', self.imp_ascending)\n",
    "\n",
    "        for i in range(5):\n",
    "            print(f'Sensitive to {_stage_label[i]}: {self.sensitive_to[i]}')\n",
    "\n",
    "        print(self.ttp)\n",
    "        \n",
    "        imp_score = [0] * len(self.imp_ascending)\n",
    "        for i, idx in enumerate(self.imp_ascending):\n",
    "            imp_score[idx] = i / (len(self.imp_ascending)-1) * 100\n",
    "\n",
    "        plt.figure(figsize=(20, 2))\n",
    "        sns.heatmap([imp_score], cmap='Reds', annot=True, cbar=True, fmt='.1f')\n",
    "        plt.title('Importance')\n",
    "        plt.xlabel('index')\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        colorlist = ['r', 'b', 'c', 'm', 'g']\n",
    "        for j in range(5):\n",
    "            for i in self.sensitive_to[j]:\n",
    "                plt.scatter(j, imp_score[i], color=colorlist[j])\n",
    "                # plt.text(j+.05, imp_score[i], f'({i}){imp_score[i]:.1f}')\n",
    "                plt.text(j+.05, imp_score[i], f'({i})')\n",
    "        plt.xlabel('sensitive to')\n",
    "        plt.xticks(range(5), _stage_label)\n",
    "        plt.ylabel('Importance')\n",
    "        plt.title('Importance / Tend to predict')\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def init_seed(cls, seed=0):\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(\n",
    "            intra_op_parallelism_threads=1,\n",
    "            inter_op_parallelism_threads=1\n",
    "        )\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    @classmethod\n",
    "    def root_mean_squared_error(cls, y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edf_hyp(dataset_path):\n",
    "    x = np.empty((0, 3600, 1))\n",
    "    y = np.empty((0,))\n",
    "    all_dir_name = sorted(glob.glob(os.path.join(dataset_path, 'shhs1-200*/')))\n",
    "    for dir_name in all_dir_name:\n",
    "        data_stage_label = dir_name[-4:-1]\n",
    "        hyp_path = os.path.join(dir_name, f'shhs1-200{str(data_stage_label).zfill(3)}_hyp_FIR.npz')\n",
    "        edf_path = os.path.join(dir_name, f'shhs1-200{str(data_stage_label).zfill(3)}_FIR.edf')\n",
    "        with pyedflib.EdfReader(edf_path) as edf:\n",
    "            hyp = np.load(hyp_path)['arr_0']\n",
    "            eeg = edf.readSignal(0).reshape((len(hyp), -1, 1))\n",
    "        x = np.concatenate([x, eeg], axis=0)\n",
    "        y = np.concatenate([y, hyp], axis=0)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = load_edf_hyp('/***/TRAIN_SET_3600/')\n",
    "x_val, y_val     = load_edf_hyp('/***/VAL_SET_3600/')\n",
    "x_test, y_test   = load_edf_hyp('/***/TEST_SET_3600/')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "y_train_oh, y_val_oh = y_train.reshape((-1, 1)), y_val.reshape((-1, 1))\n",
    "ohe                  = OneHotEncoder(categories=\"auto\", sparse=False).fit(y_train_oh)\n",
    "y_train_oh, y_val_oh = ohe.transform(y_train_oh), ohe.transform(y_val_oh)\n",
    "print(y_train_oh.shape, y_val_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('test_data', x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [32]\n",
    "fi = [5, 32]\n",
    "st = [6, 8]\n",
    "edge = 32 - 28\n",
    "\n",
    "StylizeModel.init_seed()\n",
    "inputs  = keras.layers.Input(shape=x_train.shape[1:])\n",
    "\n",
    "# Set en/decoder\n",
    "enc_1   = Conv1D(fi[0], kernel_size=ks[0], strides=st[0], padding='same', activation=LeakyReLU(), name=f'ord_{ks[0]}_1')(inputs)\n",
    "enc_2   = Conv1D(fi[0], kernel_size=ks[0], strides=st[0], padding='same', activation=LeakyAlt(),  name=f'alt_{ks[0]}_1')(inputs)\n",
    "enc_1   = Conv1D(fi[1], kernel_size=ks[0], strides=st[1], padding='same', activation=LeakyReLU(), name=f'ord_{ks[0]}_2')(enc_1)\n",
    "enc_2   = Conv1D(fi[1], kernel_size=ks[0], strides=st[1], padding='same', activation=LeakyAlt(),  name=f'alt_{ks[0]}_2')(enc_2)\n",
    "\n",
    "x       = Add()([enc_1, enc_2])\n",
    "rec     = x\n",
    "scoring = Lambda(lambda x: x[:, :, edge//2:-edge//2], name='lambda')(x)\n",
    "\n",
    "# Reconstruction\n",
    "rec     = Conv1DTranspose(fi[0], kernel_size=ks[0], strides=st[1], padding='same', activation='linear', name=f'dec_{ks[0]}')(rec)\n",
    "rec     = Conv1DTranspose(    1, kernel_size=ks[0], strides=st[0], padding='same', activation='linear', name='rec')(rec)\n",
    "\n",
    "# Stage scoring\n",
    "scoring = LayerNormalization(name='ln')(scoring)\n",
    "scoring = Conv1D(5, kernel_size=10, strides=2, padding='same', name='squeeze')(scoring)\n",
    "scoring = GlobalAveragePooling1D()(scoring)\n",
    "scoring = Softmax(name='scoring')(scoring)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[scoring, rec])\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.0005), # 0.001 or 0.0005\n",
    "                loss= {'scoring': 'categorical_crossentropy', 'rec': StylizeModel.root_mean_squared_error},\n",
    "                loss_weights={'scoring': 20., 'rec': 1.},\n",
    "                metrics={'scoring': 'accuracy'})\n",
    "\n",
    "plot_model(model, show_shapes=True, show_layer_names=False, dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.fit(x_train, {'scoring': y_train_oh, 'rec': x_train},\n",
    "              batch_size=128, epochs=100,\n",
    "              callbacks=[[EarlyStopping(monitor='val_loss', patience=3)], PlotLossesKeras()],\n",
    "              validation_data=(x_val, {'scoring': y_val_oh, 'rec': x_val}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_7746.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StylizeModel(None, None, model=model)\n",
    "m.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StylizeModel(eeg, hyp)\n",
    "# m.make_edf(dir_name='hoge')\n",
    "m.compare(idx=123, extent=25)\n",
    "m.show_rmse_transition()\n",
    "m.show_rec_transition(row=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Time evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acc. F1 transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "acc_utime = np.empty((0, 6))\n",
    "f1_utime = np.empty((0, 6))\n",
    "dir_names = sorted(glob.glob('/***/squeeze_*'), key=natural_keys)\n",
    "for i, dir in enumerate(dir_names):\n",
    "    true = np.load(os.path.join(dir, 'true.npz'))['arr_0']\n",
    "    pred = np.load(os.path.join(dir, 'pred.npz'))['arr_0']\n",
    "    cm = confusion_matrix(true, pred)\n",
    "    acc_6 = np.diag(cm) / np.sum(cm, axis=-1)\n",
    "    acc_6 = np.append(acc_6, accuracy_score(pred, true))\n",
    "    acc_utime = np.concatenate([acc_utime, acc_6.reshape((1, 6))])\n",
    "    f1_6 = f1_score(true, pred, average=None)\n",
    "    f1_6 = np.append(f1_6, np.mean(f1_6))\n",
    "    f1_utime = np.concatenate([f1_utime, f1_6.reshape((1, 6))])\n",
    "\n",
    "    if i == 25:\n",
    "        print(i)\n",
    "        print(cm)\n",
    "        print(classification_report(true, pred, target_names=_stage_label, digits=4, zero_division=0))\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(acc_utime[:, 5], '-x', label='mean')\n",
    "for i in range(5):\n",
    "    plt.plot(acc_utime[:, i], label=_stage_label[i])\n",
    "plt.xticks(range(len(acc_utime)))\n",
    "plt.xlabel('Number of latent vector removed', fontsize=20)\n",
    "plt.ylabel('U-Time Accuracy', fontsize=20)\n",
    "plt.grid()\n",
    "plt.legend(bbox_to_anchor=(1.0, 0.85, 0.3, 0.2), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(f1_utime[:, 5], '-x', label='mean')\n",
    "for i in range(5):\n",
    "    plt.plot(f1_utime[:, i], label=_stage_label[i])\n",
    "plt.xticks(range(len(f1_utime)))\n",
    "plt.xlabel('Number of latent vector removed', fontsize=20)\n",
    "plt.ylabel('U-Time F1 score', fontsize=20)\n",
    "plt.grid()\n",
    "plt.legend(bbox_to_anchor=(1.0, 0.85, 0.3, 0.2), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kappa transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Time trained by IIIS+MASS\n",
    "df = pd.read_csv('/***/evaluation_kappa.csv')\n",
    "df = df.rename(columns={'cls 0': 'W', 'cls 1': 'N1', 'cls 2': 'N2', 'cls 3': 'N3', 'cls 4': 'REM'})\n",
    "df = df.drop(len(df)-1, axis=0)\n",
    "\n",
    "df.plot(figsize=(15, 3), grid=True, style=['x--', '-', '-', '-', '-', '-'],\n",
    "        xticks=range(len(df)), xlabel='Number of latent vector removed',\n",
    "        ylim=[-0.1, 0.7], \n",
    "        ylabel='Kappa coefficient').legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ff0435f8f6dc3c9248a492d9b7521ffe816e79c8a0907874ab9b4649fa77adc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
